# -*- coding: utf-8 -*-
"""Face_MaskDetection_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ASq_kVojEOjwisvLsml_Z-XOJceIBcUI
"""

#!pip install Tensorflow==2.6.0

#!pip install keras==2.6.0

import tensorflow
import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint

import matplotlib.pyplot as plt

print(tensorflow.__version__)
print(keras.__version__)

train_data_path='/content/drive/MyDrive/Project/dataset/train'
validation_data_path='/content/drive/MyDrive/Project/dataset/valid'

#show augmented images
def plotImages(image_arr):
    fig,axes=plt.subplots(1,5,figsize=(10,10))
    axes=axes.flatten()
    for img,ax in zip(image_arr,axes):
        ax.imshow(img)
    plt.tight_layout
    #it automatically fits the graph perfectly    
    plt.show

# now calling dataaugmentation to create different type of image by flipping and others
training_datagen = ImageDataGenerator(rescale=1./255,
                                      rotation_range=40,
                                      width_shift_range=0.2,
                                      height_shift_range=0.2,
                                      shear_range=0.2,
                                      zoom_range=0.2,
                                      horizontal_flip=True,
                                      fill_mode='nearest')
tarining_data = training_datagen.flow_from_directory(train_data_path,target_size=(200,200),batch_size=128,class_mode='binary') 
#1st one says the training data path then target size is the size of image batch size means at once how many images will be taken class mode is binary because we will take binary crossentropy for loss so we need binary labels

tarining_data.class_indices

#For validation data also can be said as test data we no need to do data augmentation only scaling of data is required.
valid_datagen = ImageDataGenerator(rescale=1./255)
valid_data = valid_datagen.flow_from_directory(validation_data_path, target_size=(200,200),batch_size=128,class_mode='binary')

#now lets see some example of training data doing data augmentation
images = [tarining_data[1][0][1] for i in range(5)]
#calling plot image function 
plotImages(images)

#save model having best val accuracy ny callback library
model_path = '/content/drive/MyDrive/Project/model/model.h5'
checkpoint = ModelCheckpoint(model_path,monitor='val_accuracy',verbose=1,save_best_only=True,mode='max')
callback_list=[checkpoint]

#building CNN model
cnn_model=tensorflow.keras.models.Sequential([
                                   keras.layers.Conv2D(filters=32, kernel_size=5, input_shape=[200, 200, 3]),
                                    keras.layers.MaxPooling2D(pool_size=(4,4)),
                                    keras.layers.Conv2D(filters=64, kernel_size=4),
                                    keras.layers.MaxPooling2D(pool_size=(3,3)),
                                    keras.layers.Conv2D(filters=128, kernel_size=3),
                                    keras.layers.MaxPooling2D(pool_size=(2,2)),                                    
                                    keras.layers.Conv2D(filters=256, kernel_size=2),
                                    keras.layers.MaxPooling2D(pool_size=(2,2)),
 
                                    keras.layers.Dropout(0.5),                                                                        
                                    keras.layers.Flatten(), # neural network beulding
                                    keras.layers.Dense(units=128, activation='relu'), # input layers
                                    keras.layers.Dropout(0.1),                                    
                                    keras.layers.Dense(units=256, activation='relu'),                                    
                                    keras.layers.Dropout(0.25),                                    
                                    keras.layers.Dense(units=2, activation='softmax') # output layer
])

#compile cnn_model
cnn_model.compile(optimizer=Adam(learning_rate=0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])

#train cnn model
history = cnn_model.fit(tarining_data,
                        epochs=50,
                        verbose=1,
                        validation_data=valid_data,
                        callbacks=callback_list)

import matplotlib.pyplot as plt
# plot the loss
plt.plot(cnn_model.history.history['loss'], label='train loss')
plt.plot(cnn_model.history.history['val_loss'], label='val loss')
plt.legend()
plt.show()
plt.savefig('LossVal_loss')

#plot accuracy curve
plt.plot(cnn_model.history.history['accuracy'],label='train accuracy')
plt.plot(cnn_model.history.history['val_accuracy'],label='val accuracy')
plt.legend()
plt.show()
plt.savefig('Accval_acc')

